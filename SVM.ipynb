{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62194200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.models import word2vec\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da9720e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "nltk.download('popular')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979bda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = 'labeledTrainData.tsv'\n",
    "\n",
    "df_test = pd.read_csv(path_train_data, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bef9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppression des balises HTML \n",
    "'''\n",
    "remove_balise_html(html_doc, parser = 'html.parser')\n",
    "remove html balises on an str file\n",
    "args : \n",
    "    - html_doc = html document to clean\n",
    "    - parser\n",
    "'''\n",
    "\n",
    "\n",
    "def remove_balise_html(html_doc,parser='html.parser'):\n",
    "    cleantext = BeautifulSoup(html_doc, parser).text\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7684f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to clean a list containing the words of the sentence.\n",
    "- Replace capitals letters\n",
    "- remove punctuation\n",
    "- remove digits\n",
    "\n",
    "arg : \n",
    "    t : a list of strings \n",
    "'''\n",
    "\n",
    "\n",
    "def clean_txt(t):\n",
    "    \n",
    "    for i in range(len(t)) :\n",
    "        \n",
    "        t[i] = t[i].lower()\n",
    "        t[i] = re.sub(r'[^\\w\\s]','',t[i])\n",
    "        t[i] = ''.join([j for j in t[i] if not j.isdigit()])\n",
    "        \n",
    "    return t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87644507",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb95d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['review'] = df_test['review'].apply(remove_balise_html)\n",
    "df_test['review'] = df_test['review'].apply(lambda row : remove_stopwords(row))\n",
    "df_test['review'] = df_test['review'].apply(word_tokenize)\n",
    "df_test['review'] = df_test['review'].apply(lambda row : clean_txt(row))\n",
    "df_test['review'] = df_test['review'].apply(lambda row : list(filter(None,row))) #retrait des éléments vides\n",
    "df_test['review'] = df_test['review'].apply(lambda row : ' '.join([lemmatizer.lemmatize(w) for w in row]))\n",
    "df_test['review'] = df_test['review'].apply(lambda row : row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c564c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"My_Word2Vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382835ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return np.zeros((model.vector_size,))\n",
    "\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)\n",
    "\n",
    "def word2vec_features(X, model):\n",
    "    feats = np.vstack([sum_vectors(p, model) for p in X])\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f039493",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_train_feat = word2vec_features(df_test[\"review\"], model)\n",
    "wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf012df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wv_train_feat, \n",
    "                                                    df_test['sentiment'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb6f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_linear_svc = SVC()\n",
    "clf_linear_svc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9933b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_linear_svc.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
